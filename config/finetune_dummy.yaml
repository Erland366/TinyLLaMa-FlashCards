main:
  base_model_id: "PY007/TinyLlama-1.1B-Chat-v0.3"
  model_id_bactrian_lora: "Erland/tinyllama-1.1B-chat-v0.3-dummy-lora"
  dummy: true

bnb:
  load_in_4bit: true
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: float16
  bnb_4bit_use_double_quant: true 

hf_model:
  device_map: auto

data:
  csv_path: "datasets/dummy_anki_jsonl.csv"
 
lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM

training_arguments:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  optim: paged_adamw_32bit
  learning_rate: 0.0002
  lr_scheduler_type: cosine
  save_strategy: epoch
  logging_steps: 10
  num_train_epochs: 20
  fp16: true
  push_to_hub: true